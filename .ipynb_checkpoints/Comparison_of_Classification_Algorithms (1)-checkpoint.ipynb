{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urban Traffic in Sao Paulo Dataset\n",
    "### Cleaning up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv', delimiter=';')\n",
    "\n",
    "df[\"Slowness in traffic (%)\"] = df[\"Slowness in traffic (%)\"].map(lambda x: x.replace(',', '.')) \n",
    "time_range = df[\"Hour (Coded)\"].max() - df[\"Hour (Coded)\"].min()\n",
    "df[\"Time of Day\"] = df[\"Hour (Coded)\"].map(lambda x: 1 if x <= math.ceil(time_range/2) else -1)\n",
    "df.drop(columns = [\"Hour (Coded)\"], inplace = True)\n",
    "df[\"Slowness in traffic (%)\"] = df[\"Slowness in traffic (%)\"].astype(float)\n",
    "np_arr = df.values\n",
    "\n",
    "X = np_arr[:, :-1]\n",
    "y = np_arr[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "### Variable hyperparameters: \n",
    "    max_features: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,15\n",
    "    max_depth : 1, 2, 3, 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 3, 'max_features': 12}\n",
      "Trial 2 best parameters:{'max_depth': 4, 'max_features': 10}\n",
      "Trial 3 best parameters:{'max_depth': 2, 'max_features': 11}\n",
      "Average Train Score: 0.7373495050295006\n",
      "Average Validation Score: 0.6445987654320987\n",
      "Decision Tree using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 3, 'max_features': 15}\n",
      "Trial 2 best parameters:{'max_depth': 2, 'max_features': 13}\n",
      "Trial 3 best parameters:{'max_depth': 1, 'max_features': 12}\n",
      "Average Train Score: 0.7603077066965955\n",
      "Average Validation Score: 0.6621890547263681\n",
      "Decision Tree using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 3, 'max_features': 4}\n",
      "Trial 2 best parameters:{'max_depth': 4, 'max_features': 2}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 2}\n",
      "Average Train Score: 0.8338865955739021\n",
      "Average Validation Score: 0.6434156378600823\n",
      "Average Test Score: 0.5802469135802469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                11, 12, 13, 14,15]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                11, 12, 13, 14,15]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "                11, 12, 13, 14,15]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "for x in range(1,4):\n",
    "    dtc = DecisionTreeClassifier(max_features = bestparamsav['max_features'], max_depth = bestparamsav['max_depth'])\n",
    "    dtc.fit(X_train, y_train)\n",
    "    actual_test_scores.append(dtc.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "### Variable hyperparameters: \n",
    "    kernels = \"linear\", \"poly\", \"rbf\"\n",
    "    c = 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.1, 1, 10, 100, 1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 0.1, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 0.1, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 0.1, 'dual': True}\n",
      "Average Train Score: 0.7422904657166783\n",
      "Average Validation Score: 0.5920314253647587\n",
      "SVM using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 0.01, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 0.01, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 1, 'dual': True}\n",
      "Average Train Score: 0.6976801391162393\n",
      "Average Validation Score: 0.6492537313432837\n",
      "SVM using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 0.01, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 0.1, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 0.01, 'dual': True}\n",
      "Average Train Score: 0.6788958328325813\n",
      "Average Validation Score: 0.6306116722783389\n",
      "Average Test Score: 0.7407407407407408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.80)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.50)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.20)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    svm = LinearSVC(dual = bestparamsav['dual'], C = bestparamsav['C'])\n",
    "    svm.fit(X_train, y_train)\n",
    "    actual_test_scores.append(svm.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Variable hyperparameters: \n",
    "    c = 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4\n",
    "    penalty = l2, l1\n",
    "    class weight = balanced, none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 100, 'class_weight': None, 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 1, 'class_weight': None, 'penalty': 'l1'}\n",
      "Average Train Score: 0.6704653371320037\n",
      "Average Validation Score: 0.5925925925925927\n",
      "Logistic Regression using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 3 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Average Train Score: 0.6300332383665717\n",
      "Average Validation Score: 0.5845771144278606\n",
      "Logistic Regression using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Trial 2 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Average Train Score: 0.632247227723625\n",
      "Average Validation Score: 0.5781695156695157\n",
      "Average Test Score: 0.8148148148148148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    logr = LogisticRegression(C = bestparamsav['C'], penalty = bestparamsav['penalty'], class_weight = bestparamsav['class_weight'])\n",
    "    logr.fit(X_train, y_train)\n",
    "    actual_test_scores.append(logr.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banknote Authentication Dataset\n",
    "### Cleaning up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "banknote_df = pd.read_csv('data_banknote_authentication.txt', header = None,\n",
    "            names = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Label\"])\n",
    "banknote_df[\"Label\"] = banknote_df[\"Label\"].map(lambda x: x if x==1 else -1) \n",
    "banknote_arr = banknote_df.values\n",
    "\n",
    "X = banknote_arr[:, :-1]\n",
    "y = banknote_arr[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "### Variable hyperparameters: \n",
    "    max_features : 1, 2, 3, 4\n",
    "    max_depth : 1, 2, 3, 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Trial 2 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Average Train Score: 0.8664182590143591\n",
      "Average Validation Score: 0.8551542084472805\n",
      "Decision Tree using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 4, 'max_features': 4}\n",
      "Trial 2 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Average Train Score: 0.8662693682984345\n",
      "Average Validation Score: 0.8507045675413023\n",
      "Decision Tree using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 3, 'max_features': 3}\n",
      "Trial 2 best parameters:{'max_depth': 3, 'max_features': 2}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 4}\n",
      "Average Train Score: 0.8891973959407583\n",
      "Average Validation Score: 0.8576642335766423\n",
      "Average Test Score: 0.9350333940497876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    set_size = [1, 2, 3, 4]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    set_size = [1, 2, 3, 4]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    set_size = [1, 2, 3, 4]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "for x in range(1,4):\n",
    "    dtc = DecisionTreeClassifier(max_features = bestparamsav['max_features'], max_depth = bestparamsav['max_depth'])\n",
    "    dtc.fit(X_train, y_train)\n",
    "    actual_test_scores.append(dtc.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "### Variable hyperparameters: \n",
    "    kernels = \"linear\", \"poly\", \"rbf\"\n",
    "    c = 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.1, 1, 10, 100, 1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 100, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 1000.0, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 100, 'dual': True}\n",
      "Average Train Score: 0.839887983508796\n",
      "Average Validation Score: 0.8380889183808893\n",
      "SVM using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 2 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 3 best parameters:{'C': 10, 'dual': True}\n",
      "Average Train Score: 0.8577411188279461\n",
      "Average Validation Score: 0.8552433960597224\n",
      "SVM using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 10, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 1000.0, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 100, 'dual': True}\n",
      "Average Train Score: 0.86191051107458\n",
      "Average Validation Score: 0.8617027153946024\n",
      "Average Test Score: 0.9818181818181818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.80)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.50)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.20)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    svm = LinearSVC(dual = bestparamsav['dual'], C = bestparamsav['C'])\n",
    "    svm.fit(X_train, y_train)\n",
    "    actual_test_scores.append(svm.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Variable hyperparameters: \n",
    "    c = 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4\n",
    "    penalty = l2, l1\n",
    "    class weight = balanced, none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Average Train Score: 0.8247503337352012\n",
      "Average Validation Score: 0.8177989893318363\n",
      "Logistic Regression using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 1, 'class_weight': None, 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 3 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l2'}\n",
      "Average Train Score: 0.8251885802940648\n",
      "Average Validation Score: 0.8240636914106304\n",
      "Logistic Regression using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 3 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l2'}\n",
      "Average Train Score: 0.8313610269175852\n",
      "Average Validation Score: 0.8300259448846504\n",
      "Average Test Score: 0.9818181818181818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    logr = LogisticRegression(C = bestparamsav['C'], penalty = bestparamsav['penalty'], class_weight = bestparamsav['class_weight'])\n",
    "    logr.fit(X_train, y_train)\n",
    "    actual_test_scores.append(logr.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of Diseased Trees\n",
    "### Cleaning up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Testing.csv')\n",
    "df2 = pd.read_csv('Training.csv')\n",
    "frames = [df1, df2]\n",
    "df = pd.concat(frames)\n",
    "df.reset_index(drop=True)\n",
    "# 'w' is diseased, 'n' is all other\n",
    "df[\"class\"] = df[\"class\"].map(lambda x: 1 if x == 'w' else -1) \n",
    "np_arr = df.values\n",
    "\n",
    "X = np_arr[:, 1:]\n",
    "y = np_arr[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "### Variable hyperparameters: \n",
    "    max_features : 1, 2, 3, 4, 5\n",
    "    max_depth : 1, 2, 3, 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 4, 'max_features': 4}\n",
      "Trial 2 best parameters:{'max_depth': 4, 'max_features': 5}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 5}\n",
      "Average Train Score: 0.9600814555849561\n",
      "Average Validation Score: 0.9571945233789718\n",
      "Decision Tree using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 4, 'max_features': 5}\n",
      "Trial 2 best parameters:{'max_depth': 4, 'max_features': 5}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 4}\n",
      "Average Train Score: 0.9620404953786633\n",
      "Average Validation Score: 0.9579509439162188\n",
      "Decision Tree using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 2, 'max_features': 4}\n",
      "Trial 2 best parameters:{'max_depth': 2, 'max_features': 3}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 5}\n",
      "Average Train Score: 0.964779393359056\n",
      "Average Validation Score: 0.9571699413995173\n",
      "Average Test Score: 0.9704717630853995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "for x in range(1,4):\n",
    "    dtc = DecisionTreeClassifier(max_features = bestparamsav['max_features'], max_depth = bestparamsav['max_depth'])\n",
    "    dtc.fit(X_train, y_train)\n",
    "    actual_test_scores.append(dtc.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "### Variable hyperparameters: \n",
    "    kernels = \"linear\", \"poly\", \"rbf\"\n",
    "    c = 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.1, 1, 10, 100, 1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 2 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 3 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Average Train Score: 0.9484031478773957\n",
      "Average Validation Score: 0.9478863087963401\n",
      "SVM using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 2 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 3 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Average Train Score: 0.9499171191919629\n",
      "Average Validation Score: 0.9496536259661518\n",
      "SVM using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 2 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Trial 3 best parameters:{'C': 1000.0, 'dual': False}\n",
      "Average Train Score: 0.9492829577643036\n",
      "Average Validation Score: 0.9491009292094283\n",
      "Average Test Score: 0.9659090909090909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.80)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.50)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.20)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    svm = LinearSVC(dual = bestparamsav['dual'], C = bestparamsav['C'])\n",
    "    svm.fit(X_train, y_train)\n",
    "    actual_test_scores.append(svm.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Variable hyperparameters: \n",
    "    c = 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4\n",
    "    penalty = l2, l1\n",
    "    class weight = balanced, none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 2 best parameters:{'C': 100, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 1000.0, 'class_weight': None, 'penalty': 'l1'}\n",
      "Average Train Score: 0.9301254557474291\n",
      "Average Validation Score: 0.9290032614748233\n",
      "Logistic Regression using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 2 best parameters:{'C': 100, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 100, 'class_weight': None, 'penalty': 'l2'}\n",
      "Average Train Score: 0.932493975361295\n",
      "Average Validation Score: 0.931895994318483\n",
      "Logistic Regression using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 2 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 100, 'class_weight': None, 'penalty': 'l1'}\n",
      "Average Train Score: 0.9292693398329342\n",
      "Average Validation Score: 0.9289258059601639\n",
      "Average Test Score: 0.9628099173553718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    logr = LogisticRegression(C = bestparamsav['C'], penalty = bestparamsav['penalty'], class_weight = bestparamsav['class_weight'])\n",
    "    logr.fit(X_train, y_train)\n",
    "    actual_test_scores.append(logr.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Room Occupancy\n",
    "### Cleaning up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('datatest.txt')\n",
    "df2 = pd.read_csv('datatest2.txt')\n",
    "df3 = pd.read_csv('datatraining.txt')\n",
    "frames = [df1, df2, df3]\n",
    "df = pd.concat(frames)\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"Occupancy\"] = df[\"Occupancy\"].map(lambda x: 1 if x == 1 else -1) \n",
    "df = df.drop(columns = [\"date\"])\n",
    "\n",
    "np_arr = df.values\n",
    "np.random.shuffle(np_arr)\n",
    "X = np_arr[:2000, :-1]\n",
    "y = np_arr[:2000, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "### Variable hyperparameters: \n",
    "    max_features = 1, 2, 3, 4, 5\n",
    "    max_depth = 1, 2, 3, 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 2, 'max_features': 4}\n",
      "Trial 2 best parameters:{'max_depth': 1, 'max_features': 4}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Average Train Score: 0.9662078211472837\n",
      "Average Validation Score: 0.9643229166666667\n",
      "Decision Tree using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 1, 'max_features': 3}\n",
      "Trial 2 best parameters:{'max_depth': 3, 'max_features': 4}\n",
      "Trial 3 best parameters:{'max_depth': 3, 'max_features': 4}\n",
      "Average Train Score: 0.9659564706072514\n",
      "Average Validation Score: 0.9601833333333333\n",
      "Decision Tree using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'max_depth': 1, 'max_features': 4}\n",
      "Trial 2 best parameters:{'max_depth': 3, 'max_features': 3}\n",
      "Trial 3 best parameters:{'max_depth': 4, 'max_features': 3}\n",
      "Average Train Score: 0.9582597254684778\n",
      "Average Validation Score: 0.946875\n",
      "Average Test Score: 0.9874999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Decision Tree using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    set_size = [1, 2, 3, 4, 5]\n",
    "    max_depth = [1, 2, 3, 4]\n",
    "\n",
    "    parameters = {'max_features': set_size, 'max_depth' : max_depth}\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    search = GridSearchCV(dtc, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "for x in range(1,4):\n",
    "    dtc = DecisionTreeClassifier(max_features = bestparamsav['max_features'], max_depth = bestparamsav['max_depth'])\n",
    "    dtc.fit(X_train, y_train)\n",
    "    actual_test_scores.append(dtc.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "### Variable hyperparameters: \n",
    "    kernels = \"linear\", \"poly\", \"rbf\"\n",
    "    c = 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 0.001, 0.1, 1, 10, 100, 1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 1, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 100, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 0.1, 'dual': True}\n",
      "Average Train Score: 0.8941876262657042\n",
      "Average Validation Score: 0.8933333333333333\n",
      "SVM using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 1, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 10, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 10, 'dual': True}\n",
      "Average Train Score: 0.8994004041210976\n",
      "Average Validation Score: 0.899\n",
      "SVM using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 0.1, 'dual': True}\n",
      "Trial 2 best parameters:{'C': 100, 'dual': True}\n",
      "Trial 3 best parameters:{'C': 10, 'dual': True}\n",
      "Average Train Score: 0.901496525624941\n",
      "Average Validation Score: 0.9013636363636363\n",
      "Average Test Score: 0.9825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.80)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.50)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('SVM using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(X)\n",
    "    scaled_X = scaler.transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.20)\n",
    "\n",
    "    \n",
    "    c = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1,\n",
    "         10, 100, 1e3]\n",
    "    dual = [True, False]\n",
    "    parameters = {'dual': dual, 'C' : c}\n",
    "    svm = LinearSVC()\n",
    "    search = GridSearchCV(svm, parameters, n_jobs = -1, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    svm = LinearSVC(dual = bestparamsav['dual'], C = bestparamsav['C'])\n",
    "    svm.fit(X_train, y_train)\n",
    "    actual_test_scores.append(svm.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Variable hyperparameters: \n",
    "    c = 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4\n",
    "    penalty = l2, l1\n",
    "    class weight = balanced, none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression using a 80/20 train/test split:\n",
      "Trial 1 best parameters:{'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Trial 3 best parameters:{'C': 0.1, 'class_weight': None, 'penalty': 'l2'}\n",
      "Average Train Score: 0.9207307811088139\n",
      "Average Validation Score: 0.9196474358974359\n",
      "Logistic Regression using a 50/50 train/test split:\n",
      "Trial 1 best parameters:{'C': 10, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 2 best parameters:{'C': 0.1, 'class_weight': None, 'penalty': 'l2'}\n",
      "Trial 3 best parameters:{'C': 10000.0, 'class_weight': None, 'penalty': 'l1'}\n",
      "Average Train Score: 0.9264725320967409\n",
      "Average Validation Score: 0.9262115384615385\n",
      "Logistic Regression using a 20/80 train/test split:\n",
      "Trial 1 best parameters:{'C': 0.1, 'class_weight': None, 'penalty': 'l2'}\n",
      "Trial 2 best parameters:{'C': 100, 'class_weight': None, 'penalty': 'l1'}\n",
      "Trial 3 best parameters:{'C': 0.1, 'class_weight': None, 'penalty': 'l1'}\n",
      "Average Train Score: 0.9296917931360161\n",
      "Average Validation Score: 0.9290785256410254\n",
      "Average Test Score: 0.9899999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestparamsav = None\n",
    "bestparampersist = None\n",
    "bestval = 0\n",
    "## 80/20 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 80/20 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 50/50 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 50/50 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "## 20/80 Train/Test Split\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "actual_test_scores = []\n",
    "\n",
    "print('Logistic Regression using a 20/80 train/test split:')\n",
    "for x in range(1,4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    c = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "    penalty = ['l2', 'l1']\n",
    "    class_weight = ['balanced', None]\n",
    "    parameters = {'C' : c, 'penalty' : penalty, 'class_weight' : class_weight}\n",
    "    logr = LogisticRegression()\n",
    "    search = GridSearchCV(logr, parameters, cv=3, return_train_score=True)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_params = search.best_params_\n",
    "    bestparampersist = best_params\n",
    "    train_scores.append(search.cv_results_['mean_train_score'])\n",
    "    test_scores.append(search.cv_results_['mean_test_score'])\n",
    "    print('Trial ' + str(x) + ' best parameters:' + str(best_params))\n",
    "\n",
    "avg_train_scores = np.mean(train_scores)\n",
    "avg_test_scores = np.mean(test_scores)\n",
    "\n",
    "print(\"Average Train Score: \" + str(avg_train_scores))\n",
    "print(\"Average Validation Score: \" + str(avg_test_scores))\n",
    "if bestval < avg_test_scores:\n",
    "    bestval = avg_test_scores\n",
    "    bestparamsav = bestparampersist\n",
    "\n",
    "for x in range(1,4):\n",
    "    logr = LogisticRegression(C = bestparamsav['C'], penalty = bestparamsav['penalty'], class_weight = bestparamsav['class_weight'])\n",
    "    logr.fit(X_train, y_train)\n",
    "    actual_test_scores.append(logr.score(X_test, y_test))\n",
    "print(\"Average Test Score: \" + str(np.mean(actual_test_scores)) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
